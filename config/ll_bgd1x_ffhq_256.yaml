name: ll_2GN_bgd_64_1x_norm_adamW_1e-5_ffhq
phase: train
gpu_ids: [4,5,6,7]

# Path for logs and checkpoints
path:
  log: "logs"
  tb_logger: "tb_logger"
  results: "results"
  checkpoint: "checkpoint"
  # resume_state: null
  resume_state: experiments/ll_2GN_bgd_64_1x_norm_adamW_1e-5_ffhq_231111_112433/checkpoint/E280

# Dataset settings
datasets:
  train:
    name: FFHQ
    mode: HR
    dataroot: "data/dark/ffhq_256_256_gamma_1.5_1.7"
    batch_size: 8
    num_workers: 8
    shuffle: True
    data_len: -1
    norm:
      lr_mean: [0.0789, 0.0594, 0.0520]
      lr_std: [0.0754, 0.0638, 0.0614]
  val:
    name: CelebaHQ
    mode: LRHR
    dataroot: "data/dark/celebahq_256_256_gamma_1.5_1.7"
    batch_size: 1
    data_len: 15
    norm:
      lr_mean: [0.0789, 0.0594, 0.0520]
      lr_std: [0.0754, 0.0638, 0.0614]

# Model architecture
model:
  which_model_G: ddpm
  finetune_norm: False
  torch_compile: True

  unet:
    in_channel: 6
    out_channel: 3
    inner_channel: 64
    norm_groups: 16
    channel_multiplier: [1,1,2,2,4,4]
    attn_res: [8,16,32] # downsample, 32 means 256/32=8 actual res
    res_blocks: 1
    dropout: 0

  beta_schedule:
    train:
      schedule: cosine
      n_timestep: 1000
      linear_start: !!float 1e-6
      linear_end: !!float 1e-2
    val:
      schedule: cosine
      n_timestep: 1000
      linear_start: !!float 1e-6
      linear_end: !!float 1e-2      
      
  diffusion:
    channels: 3
    conditional: True
  

# Training
train:
  grad_accum: 1
  n_iter: 1000000
  val_epoch_freq: 10
  save_ckpt_epoch_freq: 10
  print_freq: 200

  optimizer:
    type: "adam"
    lr: !!float 1e-5
  
  ema_scheduler:
    ema_start: 1
    update_ema_every: 1
    ema_decay: 0.9999