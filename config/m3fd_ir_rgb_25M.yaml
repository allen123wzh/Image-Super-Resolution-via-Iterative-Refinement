name: up_k9srcnn_bicubic_m3fd_25M_bgd1x128_h512w384
phase: train
gpu_ids: [5]
# gpu_ids: [0]

# Adding IR to the input
ir: True

# Path for logs and checkpoints
path:
  log: "logs"
  tb_logger: "tb_logger"
  results: "results"
  checkpoint: "checkpoint"
  resume_state: m3fd_512_25M_E750
  # resume_state: /media/user/990pro/allen/sr3_server4/E700_ema_sr

# Dataset settings
datasets:
  train:
    name: m3fd_detection
    # dataroot: "data/fuse/m3fd_detect_512_384"
    dataroot: "data/fuse/m3fd_detect_512_384"
    batch_size: 32
    num_workers: 8
    shuffle: True
    data_len: -1
    norm:
      hr_mean: [0.5, 0.5, 0.5]
      hr_std: [0.5, 0.5, 0.5]
      lr_mean: [0.503, 0.507, 0.492]
      lr_std: [0.189, 0.193, 0.207]
      ir_mean: [0.33]
      ir_std: [0.198]

  val:
    name: m3fd_fusion
    dataroot: "data/fuse/m3fd_fusion_512_384"
    batch_size: 1
    data_len: -1
    norm:
      hr_mean: [0.5, 0.5, 0.5]
      hr_std: [0.5, 0.5, 0.5]
      lr_mean: [0.503, 0.507, 0.492]
      lr_std: [0.189, 0.193, 0.207]
      ir_mean: [0.33]
      ir_std: [0.198]


# Model architecture
model:
  which_model_G: ddpm
  finetune_sr: True
  torch_compile: True

  unet:
    # in_channel: 9  # RGB + IR + Noise
    in_channel: 7  # RGB + IR + Noise
    out_channel: 3
    inner_channel: 128
    norm_groups: 32
    channel_multiplier: [1,1,2,2,4,4]
    attn_res: [8,16,32]
    res_blocks: 1
    dropout: 0
  
  beta_schedule:
    train:
      schedule: cosine
      n_timestep: 1000
      linear_start: !!float 1e-6
      linear_end: !!float 1e-2
    val:
      schedule: cosine
      n_timestep: 1000
      ddim_timestep: 10
      linear_start: !!float 1e-6
      linear_end: !!float 1e-2      
      
  diffusion:
    channels: 3
    conditional: True
  

# Training
train:
  grad_accum: 1
  n_iter: 1000000
  val_epoch_freq: 100
  save_ckpt_epoch_freq: 100
  print_freq: 500

  optimizer:
    type: "adam"
    lr: !!float 1e-5
  
  ema_scheduler:
    ema_start: 5000
    update_ema_every: 1
    ema_decay: 0.9999